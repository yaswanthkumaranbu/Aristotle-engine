{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EdN0Gjs7ZL-E",
        "outputId": "582b19a7-1a58-4c60-d917-cf9f81722488"
      },
      "outputs": [
        {
          "ename": "",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31mRunning cells with 'c:\\Users\\Divyakumar\\AppData\\Local\\Programs\\Python\\Python311\\python.exe' requires the ipykernel package.\n",
            "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
            "\u001b[1;31mCommand: 'c:/Users/Divyakumar/AppData/Local/Programs/Python/Python311/python.exe -m pip install ipykernel -U --user --force-reinstall'"
          ]
        }
      ],
      "source": [
        "with open('/content/jungleBook.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "f.close()\n",
        "print(len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kMunAmnqZ19c",
        "outputId": "b1c4ec2e-6974-42bc-8cd8-d8736bbdb623"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "batch_size = 16\n",
        "block_size = 32\n",
        "eval_interval = 100\n",
        "learning_rate = 1e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 128\n",
        "n_head = 8\n",
        "n_layer = 6\n",
        "dropout = 0.3\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "\n",
        "with open('/content/jungleBook.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "def clean_text(text):\n",
        "    cleaned_text = re.sub(r'\\n', ' ', text)\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "    cleaned_text = re.sub(r'[^\\w\\s]', '', cleaned_text)\n",
        "    cleaned_text = cleaned_text.lower()\n",
        "    return cleaned_text\n",
        "original_text = text\n",
        "cleaned_text = clean_text(original_text)\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "words = cleaned_text.split()\n",
        "words = [word for word in words if word not in stop_words]\n",
        "cleaned_text = ' '.join(words)\n",
        "\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "words = cleaned_text.split()\n",
        "words = [lemmatizer.lemmatize(word) for word in words]\n",
        "cleaned_text = ' '.join(words)\n",
        "\n",
        "with open('/content/BookCleaned.txt', 'w', encoding='utf-8') as f:\n",
        "    f.write(cleaned_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dqx6RxtiaBR8",
        "outputId": "56b914c1-fdc2-4a0c-929a-1fbf222d0dee"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[' ', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', '_', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', 'Ã¢']\n"
          ]
        }
      ],
      "source": [
        "chars = sorted(list(set(cleaned_text)))\n",
        "vocab_size = len(chars)\n",
        "print(chars)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-vPg9YbaE50",
        "outputId": "3bf37d60-32d8-49c0-9f78-b31dcd94790c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.215067 M parameters\n",
            "step 0: train loss 4.5606, val loss 4.5715\n",
            "step 100: train loss 2.9921, val loss 3.1337\n",
            "step 200: train loss 2.7324, val loss 2.9137\n",
            "step 300: train loss 2.6252, val loss 2.8288\n",
            "step 400: train loss 2.5639, val loss 2.8067\n",
            "step 500: train loss 2.5191, val loss 2.7490\n",
            "step 600: train loss 2.4831, val loss 2.7568\n",
            "step 700: train loss 2.4719, val loss 2.7411\n",
            "step 800: train loss 2.4443, val loss 2.7150\n",
            "step 900: train loss 2.4403, val loss 2.7019\n",
            "step 1000: train loss 2.4086, val loss 2.7032\n",
            "step 1100: train loss 2.4008, val loss 2.6795\n",
            "step 1200: train loss 2.3897, val loss 2.6604\n",
            "step 1300: train loss 2.3619, val loss 2.6620\n",
            "step 1400: train loss 2.3502, val loss 2.6288\n",
            "step 1500: train loss 2.3362, val loss 2.6624\n",
            "step 1600: train loss 2.3270, val loss 2.6496\n",
            "step 1700: train loss 2.3083, val loss 2.6218\n",
            "step 1800: train loss 2.2911, val loss 2.6127\n",
            "step 1900: train loss 2.2906, val loss 2.5873\n",
            "step 2000: train loss 2.2738, val loss 2.5844\n",
            "step 2100: train loss 2.2683, val loss 2.5783\n",
            "step 2200: train loss 2.2517, val loss 2.5589\n",
            "step 2300: train loss 2.2270, val loss 2.5588\n",
            "step 2400: train loss 2.2078, val loss 2.5355\n",
            "step 2500: train loss 2.2077, val loss 2.5019\n",
            "step 2600: train loss 2.1862, val loss 2.5059\n",
            "step 2700: train loss 2.1744, val loss 2.4830\n",
            "step 2800: train loss 2.1609, val loss 2.4618\n",
            "step 2900: train loss 2.1416, val loss 2.4500\n",
            "step 3000: train loss 2.1342, val loss 2.4548\n",
            "step 3100: train loss 2.1071, val loss 2.4352\n",
            "step 3200: train loss 2.0973, val loss 2.4110\n",
            "step 3300: train loss 2.0947, val loss 2.4092\n",
            "step 3400: train loss 2.0789, val loss 2.4139\n",
            "step 3500: train loss 2.0720, val loss 2.4099\n",
            "step 3600: train loss 2.0587, val loss 2.3712\n",
            "step 3700: train loss 2.0465, val loss 2.3771\n",
            "step 3800: train loss 2.0372, val loss 2.3476\n",
            "step 3900: train loss 2.0230, val loss 2.3741\n",
            "step 4000: train loss 2.0199, val loss 2.3821\n",
            "step 4100: train loss 2.0119, val loss 2.3651\n",
            "step 4200: train loss 2.0041, val loss 2.3639\n",
            "step 4300: train loss 1.9917, val loss 2.3331\n",
            "step 4400: train loss 1.9848, val loss 2.3239\n",
            "step 4500: train loss 1.9799, val loss 2.3503\n",
            "step 4600: train loss 1.9707, val loss 2.3277\n",
            "step 4700: train loss 1.9492, val loss 2.3142\n",
            "step 4800: train loss 1.9363, val loss 2.3115\n",
            "step 4900: train loss 1.9370, val loss 2.3137\n",
            "step 5000: train loss 1.9316, val loss 2.3003\n",
            "step 5100: train loss 1.9214, val loss 2.3037\n",
            "step 5200: train loss 1.9226, val loss 2.3001\n",
            "step 5300: train loss 1.9135, val loss 2.2964\n",
            "step 5400: train loss 1.9176, val loss 2.3165\n",
            "step 5500: train loss 1.8891, val loss 2.2923\n",
            "step 5600: train loss 1.8837, val loss 2.2683\n",
            "step 5700: train loss 1.8901, val loss 2.2734\n",
            "step 5800: train loss 1.8830, val loss 2.2709\n",
            "step 5900: train loss 1.8773, val loss 2.2578\n",
            "step 6000: train loss 1.8727, val loss 2.2709\n",
            "step 6100: train loss 1.8623, val loss 2.2646\n",
            "step 6200: train loss 1.8697, val loss 2.2963\n",
            "step 6300: train loss 1.8490, val loss 2.2479\n",
            "step 6400: train loss 1.8646, val loss 2.2346\n",
            "step 6500: train loss 1.8379, val loss 2.2299\n",
            "step 6600: train loss 1.8372, val loss 2.2359\n",
            "step 6700: train loss 1.8292, val loss 2.2397\n",
            "step 6800: train loss 1.8273, val loss 2.2224\n",
            "step 6900: train loss 1.8216, val loss 2.2346\n",
            "step 7000: train loss 1.8129, val loss 2.2270\n",
            "step 7100: train loss 1.8128, val loss 2.2157\n",
            "step 7200: train loss 1.8139, val loss 2.2215\n",
            "step 7300: train loss 1.8035, val loss 2.2114\n",
            "step 7400: train loss 1.8075, val loss 2.2090\n",
            "step 7500: train loss 1.8042, val loss 2.2293\n",
            "step 7600: train loss 1.7921, val loss 2.2073\n",
            "step 7700: train loss 1.7942, val loss 2.2202\n",
            "step 7800: train loss 1.7797, val loss 2.2172\n",
            "step 7900: train loss 1.7845, val loss 2.1945\n",
            "step 8000: train loss 1.7840, val loss 2.1821\n",
            "step 8100: train loss 1.7678, val loss 2.1955\n",
            "step 8200: train loss 1.7741, val loss 2.1833\n",
            "step 8300: train loss 1.7709, val loss 2.1834\n",
            "step 8400: train loss 1.7587, val loss 2.1889\n",
            "step 8500: train loss 1.7599, val loss 2.1821\n",
            "step 8600: train loss 1.7607, val loss 2.1794\n",
            "step 8700: train loss 1.7476, val loss 2.2050\n",
            "step 8800: train loss 1.7562, val loss 2.1975\n",
            "step 8900: train loss 1.7336, val loss 2.1690\n",
            "step 9000: train loss 1.7442, val loss 2.1527\n",
            "step 9100: train loss 1.7347, val loss 2.1850\n",
            "step 9200: train loss 1.7391, val loss 2.1748\n",
            "step 9300: train loss 1.7340, val loss 2.1787\n",
            "step 9400: train loss 1.7285, val loss 2.1808\n",
            "step 9500: train loss 1.7299, val loss 2.1691\n",
            "step 9600: train loss 1.7212, val loss 2.1660\n",
            "step 9700: train loss 1.7355, val loss 2.1786\n",
            "step 9800: train loss 1.7297, val loss 2.1518\n",
            "step 9900: train loss 1.7225, val loss 2.1774\n",
            "step 10000: train loss 1.7130, val loss 2.1666\n",
            "step 10100: train loss 1.7178, val loss 2.1623\n",
            "step 10200: train loss 1.7024, val loss 2.1745\n",
            "step 10300: train loss 1.7073, val loss 2.1645\n",
            "step 10400: train loss 1.6950, val loss 2.1711\n",
            "step 10500: train loss 1.7017, val loss 2.1592\n",
            "step 10600: train loss 1.6923, val loss 2.1500\n",
            "step 10700: train loss 1.6931, val loss 2.1530\n",
            "step 10800: train loss 1.6972, val loss 2.1485\n",
            "step 10900: train loss 1.6906, val loss 2.1716\n",
            "step 11000: train loss 1.6862, val loss 2.1614\n",
            "step 11100: train loss 1.6930, val loss 2.1556\n",
            "step 11200: train loss 1.6796, val loss 2.1629\n",
            "step 11300: train loss 1.6789, val loss 2.1607\n",
            "step 11400: train loss 1.6807, val loss 2.1459\n",
            "step 11500: train loss 1.6759, val loss 2.1362\n",
            "step 11600: train loss 1.6641, val loss 2.1610\n",
            "step 11700: train loss 1.6635, val loss 2.1352\n",
            "step 11800: train loss 1.6644, val loss 2.1248\n",
            "step 11900: train loss 1.6659, val loss 2.1584\n",
            "step 12000: train loss 1.6609, val loss 2.1340\n",
            "step 12100: train loss 1.6655, val loss 2.1214\n",
            "step 12200: train loss 1.6612, val loss 2.1531\n",
            "step 12300: train loss 1.6576, val loss 2.1408\n",
            "step 12400: train loss 1.6637, val loss 2.1642\n",
            "step 12500: train loss 1.6588, val loss 2.1273\n",
            "step 12600: train loss 1.6463, val loss 2.1467\n",
            "step 12700: train loss 1.6376, val loss 2.1295\n",
            "step 12800: train loss 1.6481, val loss 2.1177\n",
            "step 12900: train loss 1.6537, val loss 2.1387\n",
            "step 13000: train loss 1.6404, val loss 2.1523\n",
            "step 13100: train loss 1.6371, val loss 2.1239\n",
            "step 13200: train loss 1.6430, val loss 2.1258\n",
            "step 13300: train loss 1.6374, val loss 2.1297\n",
            "step 13400: train loss 1.6425, val loss 2.1074\n",
            "step 13500: train loss 1.6312, val loss 2.1481\n",
            "step 13600: train loss 1.6314, val loss 2.1365\n",
            "step 13700: train loss 1.6352, val loss 2.1329\n",
            "step 13800: train loss 1.6288, val loss 2.1430\n",
            "step 13900: train loss 1.6304, val loss 2.1238\n",
            "step 14000: train loss 1.6190, val loss 2.1000\n",
            "step 14100: train loss 1.6222, val loss 2.1354\n",
            "step 14200: train loss 1.6136, val loss 2.1065\n",
            "step 14300: train loss 1.6195, val loss 2.1129\n",
            "step 14400: train loss 1.6141, val loss 2.1208\n",
            "step 14500: train loss 1.6187, val loss 2.1118\n",
            "step 14600: train loss 1.6117, val loss 2.0948\n",
            "step 14700: train loss 1.6119, val loss 2.0954\n",
            "step 14800: train loss 1.6167, val loss 2.1261\n",
            "step 14900: train loss 1.6209, val loss 2.0950\n",
            "step 15000: train loss 1.5977, val loss 2.1144\n",
            "step 15100: train loss 1.6178, val loss 2.1353\n",
            "step 15200: train loss 1.6008, val loss 2.1170\n",
            "step 15300: train loss 1.6060, val loss 2.1021\n",
            "step 15400: train loss 1.6015, val loss 2.1090\n",
            "step 15500: train loss 1.5948, val loss 2.1278\n",
            "step 15600: train loss 1.6038, val loss 2.1123\n",
            "step 15700: train loss 1.5957, val loss 2.1037\n",
            "step 15800: train loss 1.5952, val loss 2.1099\n",
            "step 15900: train loss 1.5822, val loss 2.1102\n",
            "step 16000: train loss 1.6007, val loss 2.1083\n",
            "step 16100: train loss 1.5833, val loss 2.1199\n",
            "step 16200: train loss 1.5944, val loss 2.1407\n",
            "step 16300: train loss 1.5799, val loss 2.0839\n",
            "step 16400: train loss 1.5865, val loss 2.1074\n",
            "step 16500: train loss 1.5821, val loss 2.1045\n",
            "step 16600: train loss 1.5773, val loss 2.1094\n",
            "step 16700: train loss 1.5804, val loss 2.1012\n",
            "step 16800: train loss 1.5773, val loss 2.1095\n",
            "step 16900: train loss 1.5808, val loss 2.0787\n",
            "step 17000: train loss 1.5768, val loss 2.1109\n",
            "step 17100: train loss 1.5756, val loss 2.1219\n",
            "step 17200: train loss 1.5643, val loss 2.1058\n",
            "step 17300: train loss 1.5757, val loss 2.0972\n",
            "step 17400: train loss 1.5782, val loss 2.0713\n",
            "step 17500: train loss 1.5688, val loss 2.0991\n",
            "step 17600: train loss 1.5676, val loss 2.0845\n",
            "step 17700: train loss 1.5611, val loss 2.0985\n",
            "step 17800: train loss 1.5636, val loss 2.0569\n",
            "step 17900: train loss 1.5628, val loss 2.1034\n",
            "step 18000: train loss 1.5712, val loss 2.1014\n",
            "step 18100: train loss 1.5570, val loss 2.0832\n",
            "step 18200: train loss 1.5688, val loss 2.0881\n",
            "step 18300: train loss 1.5635, val loss 2.1018\n",
            "step 18400: train loss 1.5580, val loss 2.0931\n",
            "step 18500: train loss 1.5541, val loss 2.0995\n",
            "step 18600: train loss 1.5563, val loss 2.1259\n",
            "step 18700: train loss 1.5509, val loss 2.0982\n",
            "step 18800: train loss 1.5399, val loss 2.0783\n",
            "step 18900: train loss 1.5439, val loss 2.0824\n",
            "step 19000: train loss 1.5534, val loss 2.1152\n",
            "step 19100: train loss 1.5406, val loss 2.0416\n",
            "step 19200: train loss 1.5455, val loss 2.1077\n",
            "step 19300: train loss 1.5410, val loss 2.0710\n",
            "step 19400: train loss 1.5393, val loss 2.0647\n",
            "step 19500: train loss 1.5438, val loss 2.0597\n",
            "step 19600: train loss 1.5486, val loss 2.0725\n",
            "step 19700: train loss 1.5433, val loss 2.0873\n",
            "step 19800: train loss 1.5479, val loss 2.0776\n",
            "step 19900: train loss 1.5373, val loss 2.0932\n",
            "step 19999: train loss 1.5378, val loss 2.1039\n",
            "tensor([[0]], device='cuda:0')\n",
            "\n",
            "tung tithers head expea than he hot the sumiders? Vit of the eart is a stup,â seep ip.â\n",
            "\n",
            "\n",
            "âI wam\n",
            "all.â Kotherrised his owed thought therrough that hold chatch swart ran and before be\n",
            "offar.â\n",
            "Akelloman, who do as he coulders grew as of of groun, fro his to lity fur, at with\n",
            "truelfalyâs\n",
            "came buffoes, ut I dido on.â He nord came amoO (to maloches\n",
            "to must his ear! Were, and the LoftoAked the our\n",
            "band a sling tof the just, and Look seake--like we never have mispelater the But Mother, hunters sporthings, Indion. Then fighter a mion.\n",
            "It curch, tran that is my their fill then in a warn to notter hurn what eaying, and the sluchf the gate tortheir rarvebâs to list undery his wards of\n",
            "him. There neest been a.â\n",
            "âMowglui. HE was nothy cubt Sra harm,â magely he man anytingo fro, milen Bacoos.\n",
            "\n",
            "The Brother,â ssaid The\n",
            "he\n",
            "shonder, butted to Ighind, little everying to thace a rust, and herd; and or the leved tungry it, any Rime in watkess of said to hiss the night fert!â\n",
            "\n",
            "So do chile onced,\n",
            "as crowly s\n"
          ]
        }
      ],
      "source": [
        "\n",
        "chars = sorted(list(set(text + '\\n')))\n",
        "vocab_size = len(chars)\n",
        "\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "max_iters = 20000\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.8*len(data))\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd)\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        for _ in range(max_new_tokens):\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            logits, loss = self(idx_cond)\n",
        "            logits = logits[:, -1, :]\n",
        "            probs = F.softmax(logits, dim=-1)\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)\n",
        "            idx = torch.cat((idx, idx_next), dim=1)\n",
        "        return idx\n",
        "\n",
        "model = BigramLanguageModel()\n",
        "m = model.to(device)\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "for iter in range(max_iters):\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "    xb, yb = get_batch('train')\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(context)\n",
        "print(decode(m.generate(context, max_new_tokens=1000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S3HBrDhWqxil",
        "outputId": "d64fe11c-fc0b-43ef-942a-d939859a11e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Generated Answer: What title book\n",
            "asoomil the Jungle to throge, scame that that Colf af deay the Council\n",
            "nomarlswed floom\n",
            "to foor cal\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import torch\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "stop_words = set(stopwords.words('english'))\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def clean_text(text):\n",
        "    cleaned_text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    cleaned_text = cleaned_text\n",
        "    return cleaned_text\n",
        "\n",
        "def generate_answer(question):\n",
        "    cleaned_question = clean_text(question)\n",
        "    words = cleaned_question.split()\n",
        "    words = [word for word in words if word not in stop_words]\n",
        "    lemmatized_words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    cleaned_question = ' '.join(lemmatized_words)\n",
        "\n",
        "    question_data = torch.tensor(encode(cleaned_question), dtype=torch.long, device=device).unsqueeze(0)\n",
        "    context = question_data\n",
        "    answer = decode(m.generate(context, max_new_tokens=100)[0].tolist())\n",
        "    return answer\n",
        "\n",
        "question = \"What is the title of this book?\"\n",
        "answer = generate_answer(question)\n",
        "print(\"Generated Answer:\", answer)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
